

PROJECTS ARE DUE TODAY !!


### GPT Questions
Replay Buffer?
Sampling data from replay buffer?



## Recap

MC evaluation with NN
Bootstrapping 
- Instead of waiting for the entire episode, it uses a **value estimate** of the next state. This estimate is based on previous knowledge or calculations.
- instead of sum of rewards, bootstrapping uses value estimate  + only the reward at the current time $t$
- $V(s_t) = R_t + Î³ * V(s_{t+1})$

## Tracing n-steps

Traces are useful strategy
however if too many steps are taken the variance between paths blows up, thus small time steps are taken b/t bootstrapping steps to reduce variance



## Parallelization
an online AC algorithm 
see slide 13

### Sync vs Async 
Async works best faster than synch
see slide 14

### AC with a Replay Buffer

Definition: a replay buffer is a collection of events that are saved a tuples: (state, action, reward, next state, done) $[a_i, s_i, r_i, s'_i]$
off policy learning
- Don't sample from the Buffer directly bc the current policy theta hasn't made those decisions

## Policy Iteration (concept)

- Seek to improve policy $\pi$ to become $\pi'$ by maxing the advantage $argmax [A(s_t, a_t)]$

> Remember: Advantage (A) is a measure of how much better an action is compared to the average action:
> 
	$A(s, a) = Q(s, a) - V(s)$

#### What does argmax(A) mean?
- Compare the objectives at the time t and t' to the find the \theta prime that has the max A(s, a) this is written as:

	$J(\theta') - J(\theta)$
## Advanced Policy Gradient Methods

### Trust Region Policy Optimization (TRPO)

- Constrains $\pi$ and $\pi'$ by KL divergence (the trust region)
- Calculate the trajectories (from several $\pi'$ s) and use backtracking to find the best $\pi'$ policy that results in the biggest improvement in Advantage $A(s,a)$

Cons:
* slower, can big O complex

### PPO
* most popular policy grad algo
* Faster, more efficient, more performant
* Uses pessimistic ***clipping***?
	* Clips overly optimizing policy estimates
	* Is pessimistic when L (the ratio) is negative
image from slide 30...


## Value-Based methods

> What if there's no parameterized policy?

### Value-based

#### Policy Iteration (old method)
Evaluate the Adv (2) update new policy $\pi=\pi'$ (3) repeat
- assuming you have transition matrix T.shape(16x16x4)
- need to know the transition matrix to calculate the Expectation ($E$)

> what is transition matrix T
> It is the probability that an agent

#### Value iteration vs Policy iteration
- In these versions $\pi$ is not an NN $\pi$ is more like the SoftMax of Q's for an action space. e.g. $\pi = [Q_{up}, Q_{down}, ...]$
- Q-Learning DOES NOT need knowledge of the transition matrix (T)
- Q-learning allows for off-policy learning

> What is off policy learning?
> **Off-policy learning**, allows the agent to learn from experiences generated by a different policy

### Q-Learning

**Recap**: Q value is the quality of an action taken in a given state (e.g. moving paddle up has a high Q-value when a tennis ball is headed towards to agent)

#### Fitting Q 

Learn a Q function, then 

```python
while not converged:
	collect data D=[a_i, s_i, r_i, s'_i]:  # from a policy pi
		# calculate target y
		y_i = r(s, a)
		
		# use phi to inform next loop
		phi = min(Q(s, a) - y_i)**2
```

Q-learning does not guarantee convergence (mathematically speaking)

What are we optimizing, how do we know we are good

#### Bellman error ($\epsilon$)
see slides and GPT...

#### Online Q-Learning

```python
while not converged:
	collect data D=[a_i, s_i, r_i, s_i_prime]:  # from a policy pi
		# calculate target y
		y_i = r(s, a)
		
		# use phi to inform next loop
		phi = phi - alpha * (Q(s, a) - y_i).detivative_wrt_Q
```


## Code Implementation of HW


`configs/` : 
- env info, hyper params, not really needed to modify to complete HW

`common/`: 
- Modules used for RL NN building, and categorical sampling in pytorch but neat to read but don't modify

`rl/templates` : 
- MODIFY THIS
- in the code look for `todo` train, fwd pass, collect data, etc..
- homework thought process, use debugger, use breakpoints by the `todos` to look at the vars you have, what you may need



Mejoku Environments

---
## Jargon to English

$x \sim f(x)$: 
- SpokenðŸ™Š: x sampled from distribution - f(x)

$Ï€_Î¸(a | s)$: 
- SpokenðŸ™Š: probability of taking action - $a$ given state - $s$ under policy - $Ï€_Î¸$