
Tags: [[Parameter Efficient Fine Tuning]]

Institution: [[Meta AI Research]]

Links: [arxiv.org](https://arxiv.org/abs/2012.13255)

Key Authors: [[Armen Aghajanyan]], [[Luke Zettlemoyer]], [[Sonal Gupta]]

Year: 2020


| ***What they did? (overview)***       | ***What was the result?***                                                                                                                                                            |
| ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| content...                            | By optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90\% of the full parameter performance levels on MRPC |
| ***How they did it? (method brief)*** | ***Conclusions / Drawbacks***                                                                                                                                                         |
| content...                            | content...                                                                                                                                                                            |


### Main Idea

content...

### Technical Details or Model Architecture

* content...

### Conclusion / Discussion

content...

### Gaps in Lit

content...


---
### References

[^1]: [[PAPER LINK GOES HERE]]

