Tags: [[OpenAI]], [[Multimodal Models]]

**CLIP** which stands for **Contrastive Language-Image Pre-training**

CLIP is a **joint** image and text [embedding](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture) model trained using 400 million image and text pairs in a self supervised way. This means that it maps both text and images to the same embedding space

Note: that CLIP is **_not_** a **_caption generation_** model, it can only tell you if some existing text caption fits well with an existing image or not.